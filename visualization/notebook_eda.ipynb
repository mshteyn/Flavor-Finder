{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "057cbab3-4d3f-4a34-9bc4-94e13e3f269a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mshteyn/anaconda3/envs/huggingface/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original array shape (transposed): (1024, 1800)\n"
     ]
    }
   ],
   "source": [
    "#tsne \n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "#from langchain_chroma import Chroma\n",
    "import torch\n",
    "import pdb\n",
    "import os\n",
    "from langchain import HuggingFacePipeline\n",
    "#from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "model_name  = 'Alibaba-NLP/gte-large-en-v1.5'\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=model_name, model_kwargs = {'trust_remote_code':True, 'device': torch.device(\"cpu\")})\n",
    "     \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#from sklearn.datasets import fetch_mldata\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "chroma_dir = '../scripts/PA_200c_named_db'\n",
    "db = Chroma(persist_directory=chroma_dir, embedding_function=embedding_function)\n",
    "\n",
    "# docs=db.similarity_search('what is the best chicken in Mcdonald')\n",
    "# print(docs)\n",
    "     \n",
    "\n",
    "#vdb=db.get(include=[\"metadatas\", \"documents\", \"embeddings\"])\n",
    "#vdb=db.get(include=[\"embeddings\"])\n",
    "import pandas as pd\n",
    "#vdb=pd.DataFrame(db)\n",
    "#df = pd.DataFrame({\"embeddings\": vdb[\"embeddings\"],})\n",
    "vectors=db.get(include=[\"embeddings\", \"documents\"])\n",
    "# print(vectors)\n",
    "# Convert the embeddings to a numpy array and transpose it\n",
    "vec_array = np.asarray(vectors['embeddings'])\n",
    "txt_array = vectors['documents']\n",
    "num_test = 1800\n",
    "#pdb.set_trace()\n",
    "select_randidx = np.random.randint(0,len(vectors['embeddings']),num_test)\n",
    "vec_array = vec_array[select_randidx, :]\n",
    "print(\"Original array shape (transposed):\", vec_array.T.shape)\n",
    "\n",
    "\n",
    "datafile = '../data/200chrestReviews_PA.json'\n",
    "df = pd.read_json(datafile, lines=True)\n",
    "meta_df = pd.read_json('../data/meta-Pennsylvania.json', lines=True)\n",
    "\n",
    "#search for query to get gmap id\n",
    "relevant_metadata = ['Noodle', 'Thai'] \n",
    "noodle_list, thai_list = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34797f8-bcb1-4e0a-af92-9b33743dc57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_18616/2827759088.py\u001b[0m(14)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     12 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     13 \u001b[0;31m    \u001b[0;31m#string_meta = \" \".join(entry.category.tolist()[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 14 \u001b[0;31m    \u001b[0mstring_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     16 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0mrelevant_metadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring_meta\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  item\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"We got food at Eat'n Park. It was my first time in a restaurant since covid 19 shut things down  My breakfast was ok gotten eggs Benedict  It would of been a lot nicer if the people at the next table didn t talk about bodily fluids   talk lower to themselves  So pretty much couldn t enjoy my breakfast cause people talking about bodily fluids etc\"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  ii\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  meta_df[meta_df['gmap_id'] == get_gmap].reset_index().iloc[0].category\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** ValueError: Can only compare identically-labeled Series objects\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  get_gmap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30    0x89cb11cddc8f9cdd:0xa6b8e58541d8967c\n",
      "Name: gmap_id, dtype: object\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  meta_df['gmap_id'] == get_gmap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** ValueError: Can only compare identically-labeled Series objects\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  get_gmap = df[find_idx]['gmap_id']\n",
      "ipdb>  get_gmap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30    0x89cb11cddc8f9cdd:0xa6b8e58541d8967c\n",
      "Name: gmap_id, dtype: object\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  df[find_idx]['gmap_id']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30    0x89cb11cddc8f9cdd:0xa6b8e58541d8967c\n",
      "Name: gmap_id, dtype: object\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  df[find_idx]['gmap_id'].reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index                                gmap_id\n",
      "0     30  0x89cb11cddc8f9cdd:0xa6b8e58541d8967c\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  df[find_idx]['gmap_id'].reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** TypeError: Cannot reset_index inplace on a Series to create a DataFrame\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  df[find_idx]['gmap_id']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30    0x89cb11cddc8f9cdd:0xa6b8e58541d8967c\n",
      "Name: gmap_id, dtype: object\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  df[find_idx]['gmap_id'].copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.copy of 30    0x89cb11cddc8f9cdd:0xa6b8e58541d8967c\n",
      "Name: gmap_id, dtype: object>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  df[find_idx]['gmap_id'].copy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30    0x89cb11cddc8f9cdd:0xa6b8e58541d8967c\n",
      "Name: gmap_id, dtype: object\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  df[find_idx]['gmap_id'].copy().reset_index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Series.reset_index of 30    0x89cb11cddc8f9cdd:0xa6b8e58541d8967c\n",
      "Name: gmap_id, dtype: object>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  df[find_idx]['gmap_id'].copy().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index                                gmap_id\n",
      "0     30  0x89cb11cddc8f9cdd:0xa6b8e58541d8967c\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  df[find_idx]['gmap_id'].copy().reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** TypeError: Cannot reset_index inplace on a Series to create a DataFrame\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  df[find_idx]['gmap_id'].copy().reset_index(drop=True, inplace=True)\n",
      "ipdb>  gmap_id = df[find_idx]['gmap_id'].copy().reset_index(drop=True, inplace=True)\n",
      "ipdb>  gmap_id\n",
      "ipdb>  gmap_id\n",
      "ipdb>  type(gmap_id)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "relevant_metadata = ['Noodle', 'Thai'] \n",
    "noodle_list, thai_list = [], []\n",
    "for ii, item in enumerate(vectors[\"documents\"]):\n",
    "    find_idx = df['text'].str.contains(item[-20:-1])\n",
    "    get_gmap = df[find_idx]['gmap_id']\n",
    "    if len(get_gmap)>1:\n",
    "        get_gmap = get_gmap.iloc[df.index[0]]        \n",
    "    #entry = meta_df[meta_df['gmap'] == get_gmap]\n",
    "    try:\n",
    "        categories = meta_df[meta_df['gmap_id'] == get_gmap].reset_index().iloc[0].category\n",
    "    except:\n",
    "        pdb.set_trace()\n",
    "    #string_meta = \" \".join(entry.category.tolist()[0])\n",
    "    string_meta = \" \".join(categories)\n",
    "    \n",
    "    if relevant_metadata[0] in string_meta:\n",
    "        noodle_list.append(ii)\n",
    "    if relevant_metadata[1] in string_meta:\n",
    "        thai_list.append(ii)\n",
    "    \n",
    "pdb.set_trace()\n",
    "\n",
    "\n",
    "# Save the transposed array to a text file\n",
    "# In future add request vector, convenient for plotting\n",
    "text_file_name = 'array_small.text'\n",
    "\n",
    "np.savetxt(text_file_name, vec_array.T, delimiter=',', fmt='%.18e')\n",
    "\n",
    "# Load the array from the text file\n",
    "loaded_array = np.loadtxt(text_file_name, delimiter=',')\n",
    "     \n",
    "\n",
    "## add query vector in\n",
    "from sentence_transformers import SentenceTransformer\n",
    "query=['For the best vegetarian plate, what should I order on the menu at Noodlehead?']\n",
    "\n",
    "#model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "#model = model\n",
    "model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "query_embeddings = model.encode(query)\n",
    "#print(query_embeddings.shape)\n",
    "# Prepare documents for Chroma.from_documents\n",
    "query_vec = query_embeddings\n",
    "query_vec_array=np.asarray(query_vec)\n",
    "\n",
    "#print(query_vec_array.T.shape)\n",
    "#loaded_array = np.random.rand(384, 1)\n",
    "##loaded_array = np.loadtxt('array.txt', delimiter=',')\n",
    "#print(loaded_array.shape)\n",
    "updated_array = np.hstack([loaded_array, query_vec_array.T])\n",
    "np.savetxt(text_file_name, updated_array, delimiter=',', fmt='%.18e')\n",
    "#loaded_array = np.loadtxt('array.txt', delimiter=',')\n",
    "#print(\"Updated array shape:\", loaded_array.shape)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96afdb80-aae7-49a7-b14f-df1924f694ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "loaded_array = np.loadtxt(text_file_name, delimiter=',')\n",
    "n_components = 2 #3D\n",
    "# tsne = TSNE(n_components=n_components, random_state=1, perplexity=1)\n",
    "# reduced_vectors = tsne.fit_transform(loaded_array)\n",
    "     \n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# # Plot the reduced embeddings\n",
    "# plt.scatter(reduced_vectors[:,  0], reduced_vectors[:,  1], c='red', alpha=0.2)\n",
    "\n",
    "# # Add labels and title\n",
    "\n",
    "# plt.show()\n",
    "     \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Directory to save figures\n",
    "save_dir = 'tsne_figures'\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#parameter for tSNE list\n",
    "#perplexity_col = [2,4,8,10]\n",
    "perplexity_col=[50,80,100,200,300]\n",
    "random_state_col=[10,20,40]\n",
    "\n",
    "\n",
    "# Save figures in a loop\n",
    "for i in range(len(perplexity_col)):\n",
    "    for j in range(len(random_state_col)):\n",
    "        tsne = TSNE(n_components=n_components, random_state=random_state_col[j], perplexity=perplexity_col[i])\n",
    "        reduced_vectors = tsne.fit_transform(loaded_array.T)\n",
    "        plt.figure()\n",
    "        plt.scatter(reduced_vectors[:-1, 0], reduced_vectors[:-1,  1], c='purple', alpha=0.2)\n",
    "        plt.scatter(reduced_vectors[-1, 0], reduced_vectors[-1,  1], marker ='*', c='red')\n",
    "        filename = os.path.join(save_dir, f'figure_perplex[{perplexity_col[i]}]_randomstate[{random_state_col[j]}].png')  # Create a filename with the loop index\n",
    "        plt.title(\"t-SNE Visualization of Text Embeddings with Question Highlighted\")\n",
    "        plt.xlabel(\"t-SNE  1\")\n",
    "        plt.ylabel(\"t-SNE  2\")        \n",
    "        plt.savefig(filename)  # Save the figure\n",
    "        plt.close()  # Close the figure to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0f658-2c77-4314-966c-6f50d2c36218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
